<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://preaction.github.io/blog/tag/software/index.html</id>
    <title>Doug Bell</title>
    <updated>2014-06-22T02:20:15Z</updated>
    <link rel="self" href="http://preaction.github.io/blog/tag/software.atom"/>
    <link rel="alternate" href="http://preaction.github.io/blog/tag/software/index.html"/>
    <generator version="0.013">Statocles</generator>
    <entry>
        <id>http://preaction.github.io/blog/2012/11/04/testing-is-a-feature-of-your-service.html</id>
        <title>Testing is a Feature of Your Service</title>
        <author><name>preaction</name></author>
        <link rel="alternate" href="http://preaction.github.io/blog/2012/11/04/testing-is-a-feature-of-your-service.html" />
        <content type="html"><![CDATA[
            <p>My job at Bank of America consists largely of data collection and storage. To
collect data in Perl, I have to write XS modules to interface with the
vendor-supplied native libraries. Because I want to know my code works, my XS
modules come with robust test suites, testing that everything works correctly.</p>

<p>Since the XS module was intended to be used by other, larger systems, I decided
to help those larger systems test their dependency on my module: I included a
<a href="http://search.cpan.org/perldoc?Test::MockObject">Test::MockObject</a> that mocked
my module's interface. By using my test module, the tests can try some data and
see if their code works.</p>

<p>But the hardest part to test is always the failures. How do they test if the
news service goes down in the middle of a data pull? How about if it goes down
between data pulls but still inside the same process? How do they test if the
user has input an invalid ID for data?</p>

<hr />

<p>To help them write good error-checking and recovery, I added specific
Test::MockObjects that return failure conditions. Want to know what happens if
an ID is invalid? Use the Test::Feed::InvalidId mock API. Want to know what
happens if the feed goes down in the middle of a process? Use the
Test::Feed::RandomDisconnect mock API.</p>

<p>By providing these mock objects, users can write more robust code more simply.
Then, in the eventuality that the service fails, they know exactly what their
code won't do: Wake them up at 3 in the morning because of an unexpected,
unhandled error. These mock objects don't even need an external connection to
operate, so they can be as fast as possible.</p>

<p>Unfortunately, this is a limited solution that covers only what I know about.
If the service changes, or has an error I didn't cover, the code still fails.
For this reason, for the web services I build, I build in explicit ways to get
errors. When the service itself gives the exact error output it gives in
exceptional circumstances, it can be tested and handled. Instead of a mock
service, it's the real service (perhaps on a dev system), with real input,
returning a real (but desired) error message.</p>

<p>By providing as much help as possible to the users of my code, I can make sure
they can create robust applications that make their own users happy. By helping
users respond to and deal with the error conditions of my code, I can cut down
on the support requests and bug reports. Being proactive about testing helps
everyone write better code.</p>

        ]]></content>
        <updated>2012-11-04T00:00:00Z</updated>
        <category term="ARRAY(0x7fe715a73f00)" />
    </entry>
    <entry>
        <id>http://preaction.github.io/blog/2012/09/23/plan-software-to-live-forever.html</id>
        <title>Chicago.PM - Beyond grep - Expanding the Programmer Toolset</title>
        <author><name>preaction</name></author>
        <link rel="alternate" href="http://preaction.github.io/blog/2012/09/23/plan-software-to-live-forever.html" />
        <content type="html"><![CDATA[
            <p>How often have I told myself, "I'll kludge this now and rewrite it later"? And
how many times did I actually go back and rewrite that kludgy bit? "Too often"
and "not enough". Many job postings include the phrase "update legacy
applications," as a euphemism for "rewrite poorly-designed spaghetti." The Y2K
problem was a huge exercise in code out-living the developer's plan, with a
healthy dose of cargo-culting thrown in. Lately, I've been learning to plan for
a likely possibility: My code will survive to haunt my bug lists and my resume
for a long time.</p>

<hr />

<p>We developers are a lazy lot, it's one of our greatest strengths. But like all
virtues, it is a double-edged sword that must be wielded responsibly. Laziness
is defined as maximum gain for minimal effort. For laziness's sake, I write
automated tests to avoid manual testing, I build a development server that
mimics production closely so I don't get woken up at 2:00 am when my code blows
up, I write documentation that explains the design of the software so I don't
have to trace through layers of code to figure it out, and anything else I can
do to make sure I don't have to work as hard solving the same problem in the
future.</p>

<p>Automated tests are the programmer's best friend. How do I know my code works?
By running it. How do I know it works every time I change something? By running
it again. Automated tests are simply running the code, controlling the input
and checking the output. Since I write tests once according to expected input
and output, I can refactor the entire program and be absolutely sure it works
for that set of input. I don't need to test manually, clicking everywhere or
preparing input on-the-fly, which saves me a lot of time in the long run
(because this code will live forever).</p>

<p>A development server is another necessity for laziness. At a former employer I
instituted a policy: All custom client projects included renting a
development/staging server, no exceptions. This staging server could be
completely destroyed and rebuilt with zero consequences. Our deployment
processes were automated (more laziness) to make it easy to recover from
mistakes. Once our automated tests passed, the clients had a place they could
verify the expected behavior and see their ideas in action before releasing the
code on to an important, production system. We instituted this policy from
learning our lesson: Our laptops were never the same as the production server,
so we needed some place that was. Deploying code directly to production is only
rarely without unforeseen consequences.</p>

<p>I've been called a hero a couple times now. I don't believe or understand why,
but I've noticed it's always after I mention how much I enjoy writing
documentation. Writing documentation is like explaining my code to someone
else, usually future me: It forces me to think critically about the code, how
it works, and what side-effects and edge-cases it has. Every time I go back to
write documentation I've found bugs in my code: incorrect assumptions,
inelegant algorithms, or undesired side-effects. Usually, the first other
person who reads them finds even more: things I didn't think of, or things I
thought would never be a problem (famous last words). Writing the documentation
gets me out of the code mindset and into the design mindset, and well-designed
code is code that will live forever.</p>

<p>There is nothing more boring than a solved problem. By making sure I have
automated tests, a sandbox environment, good documentation, I can make
maintenance easier, which is the true definition of laziness: Maximum gain for
minimum effort.</p>

        ]]></content>
        <updated>2012-09-23T00:00:00Z</updated>
        <category term="ARRAY(0x7fe7140df4a0)" />
    </entry>
</feed>

